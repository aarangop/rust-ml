diff --git a/.gitattributes b/.gitattributes
new file mode 100644
index 0000000..60404dc
--- /dev/null
+++ b/.gitattributes
@@ -0,0 +1 @@
+*.ipynb linguist-documentation
\ No newline at end of file
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index 227d87f..76af861 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@ -14,28 +14,31 @@ jobs:
         with:
           fetch-depth: 0
 
-      - name: Extract tag name
-        id: tag_name
-        run: echo "TAG_NAME=${GITHUB_REF#refs/tags/}" >> $GITHUB_ENV
-      - name: Generate simple changelog
+      - name: Generate changelog
+        id: changelog
         run: |
           # Get the latest tag before this one
           PREVIOUS_TAG=$(git describe --tags --abbrev=0 HEAD^ 2>/dev/null || echo "")
 
           if [ -z "$PREVIOUS_TAG" ]; then
             # If no previous tag exists, get all commits
-            git log --pretty=format:"- %s" > changelog.md
+            echo "CHANGELOG<<EOF" >> $GITHUB_OUTPUT
+            git log --pretty=format:"- %s" >> $GITHUB_OUTPUT
+            echo "EOF" >> $GITHUB_OUTPUT
           else
             # Get commits between the previous tag and this one
-            git log --pretty=format:"- %s" ${PREVIOUS_TAG}..HEAD > changelog.md
+            echo "CHANGELOG<<EOF" >> $GITHUB_OUTPUT
+            git log --pretty=format:"- %s" ${PREVIOUS_TAG}..HEAD >> $GITHUB_OUTPUT
+            echo "EOF" >> $GITHUB_OUTPUT
           fi
 
       - name: Create Release
-        uses: softprops/action-gh-release@v2
-        if: github.ref_type == 'tag'
+        uses: softprops/action-gh-release@v1
         with:
           name: Release ${{ github.ref_name }}
-          body_path: changelog.md
+          body: |
+            ## Changelog
+            ${{ steps.changelog.outputs.CHANGELOG }}
           draft: false
           prerelease: false
         env:
diff --git a/CHANGELOG.md b/CHANGELOG.md
index 4519ce3..7be4215 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -6,6 +6,12 @@ The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
 and this project adheres to
 [Semantic Versioning](https://semver.org/spec/v2.0.0.html).
 
+## [Unreleased]
+
+### Added
+
+- Add single hidden layer classification model
+
 ## [0.1.5] - 2025-04-18
 
 ### Changed
diff --git a/notebooks/single_layer_classifier.ipynb b/notebooks/single_layer_classifier.ipynb
new file mode 100644
index 0000000..288803f
--- /dev/null
+++ b/notebooks/single_layer_classifier.ipynb
@@ -0,0 +1,266 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "id": "1a063a78",
+   "metadata": {},
+   "source": [
+    "# Single Hidden Layer Neural Network\n",
+    "\n",
+    "This notebook implements a simple single hidden layer neural network from scratch using NumPy."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 16,
+   "id": "7da842b7",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import numpy as np"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "id": "748c7ac7",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def init_params(n_features, n_hidden_nodes):\n",
+    "    \"\"\"\n",
+    "    Initialize the parameters for a neural network with one hidden layer.\n",
+    "\n",
+    "    Args:\n",
+    "        n_features (int): Number of input features.\n",
+    "        n_hidden_nodes (int): Number of hidden nodes in the hidden layer.\n",
+    "\n",
+    "    Returns:\n",
+    "        dict: A dictionary containing the initialized weights and biases.\n",
+    "    \"\"\"\n",
+    "\n",
+    "    # Initialize weights and biases\n",
+    "    W1 = np.random.randn(n_hidden_nodes, n_features) * 0.01  # Weights for input to hidden layer\n",
+    "    b1 = np.zeros((n_hidden_nodes, 1))  # Biases for hidden layer\n",
+    "    W2 = np.random.randn(1, n_hidden_nodes) * 0.01  # Weights for hidden to output layer\n",
+    "    b2 = np.zeros((1, 1))  # Biases for output layer\n",
+    "\n",
+    "    return {\n",
+    "        'W1': W1,\n",
+    "        'b1': b1,\n",
+    "        'W2': W2,\n",
+    "        'b2': b2\n",
+    "    }"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 18,
+   "id": "95a303ed",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def sigmoid(z):\n",
+    "    \"\"\"\n",
+    "    Compute the sigmoid activation function.\n",
+    "\n",
+    "    Args:\n",
+    "        z (np.ndarray): Input array.\n",
+    "\n",
+    "    Returns:\n",
+    "        np.ndarray: Sigmoid of the input array.\n",
+    "    \"\"\"\n",
+    "    \n",
+    "    return 1 / (1 + np.exp(-z))\n",
+    "\n",
+    "def forward_propagation(X, params):\n",
+    "    \"\"\"\n",
+    "    Perform forward propagation through the neural network.\n",
+    "\n",
+    "    Args:\n",
+    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
+    "        params (dict): Dictionary containing the weights and biases.\n",
+    "\n",
+    "    Returns:\n",
+    "        np.ndarray: Output of the neural network.\n",
+    "    \"\"\"\n",
+    "    \n",
+    "    W1 = params['W1']\n",
+    "    b1 = params['b1']\n",
+    "    W2 = params['W2']\n",
+    "    b2 = params['b2']\n",
+    "    print(\"W1.shape:\", W1.shape)\n",
+    "    # Hidden layer\n",
+    "    Z1 = W1 @ X + b1\n",
+    "    A1 = sigmoid(Z1)  # Activation function\n",
+    "\n",
+    "    # Output layer\n",
+    "    Z2 = W2 @ A1 + b2\n",
+    "    A2 = sigmoid(Z2)  # Linear activation for output layer\n",
+    "\n",
+    "    return A2\n",
+    "\n",
+    "def compute_loss(y_true, y_pred):\n",
+    "    \"\"\"\n",
+    "    Compute the mean squared error loss.\n",
+    "\n",
+    "    Args:\n",
+    "        y_true (np.ndarray): True labels.\n",
+    "        y_pred (np.ndarray): Predicted labels.\n",
+    "\n",
+    "    Returns:\n",
+    "        float: Mean squared error loss.\n",
+    "    \"\"\"\n",
+    "    \n",
+    "    m = y_true.shape[0]\n",
+    "    loss = - (y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
+    "    cost = np.sum(loss) / m\n",
+    "    return cost\n",
+    "\n",
+    "def predict(X, params):\n",
+    "    \"\"\"\n",
+    "    Make predictions using the trained neural network.\n",
+    "\n",
+    "    Args:\n",
+    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
+    "        params (dict): Dictionary containing the weights and biases.\n",
+    "\n",
+    "    Returns:\n",
+    "        np.ndarray: Predicted labels.\n",
+    "    \"\"\"\n",
+    "    \n",
+    "    A2 = forward_propagation(X, params)\n",
+    "    predictions = (A2 > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
+    "    return predictions"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 55,
+   "id": "64199600",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "X.shape: (2, 2)\n",
+      "y.shape: (1, 2)\n"
+     ]
+    }
+   ],
+   "source": [
+    "X = np.array([1, 2, 3, 4]).reshape(2, 2)  # Example input\n",
+    "y = np.array([1, 0]).reshape(1, 2)  # Example output\n",
+    "print(\"X.shape:\", X.shape)\n",
+    "print(\"y.shape:\", y.shape)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 56,
+   "id": "453f9dec",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "n_features = X.shape[0]\n",
+    "n_hidden_nodes = 3\n",
+    "m = X.shape[1]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 60,
+   "id": "0e57c115",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "params['W1'].shape: (3, 2)\n"
+     ]
+    }
+   ],
+   "source": [
+    "params = init_params(n_features, n_hidden_nodes)  # Initialize parameters\n",
+    "# Set params to fixed numbers for deterministic behavior\n",
+    "params['W1'] = np.array([0.1, 0.2, 0.3, 0.4, 5.0, 6.0]).reshape(3, 2)\n",
+    "params['b1'] = np.array([0.1, 0.2, 0.3]).reshape(3, 1)\n",
+    "params['W2'] = np.array([[0.1, 0.2, 0.3]])\n",
+    "params['b2'] = np.array([[0.1]])\n",
+    "\n",
+    "print(\"params['W1'].shape:\", params['W1'].shape)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 61,
+   "id": "5755fb65",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "W1.shape: (3, 2)\n",
+      "Output activation: [[0.65432483 0.65889896]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "a2 = forward_propagation(X, params)  # Forward propagation\n",
+    "print(\"Output activation:\", a2)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 62,
+   "id": "bc80348c",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "W1.shape: (3, 2)\n",
+      "Predictions: [[1 1]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "predictions = predict(X, params)  # Make predictions\n",
+    "print(\"Predictions:\", predictions)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "35ad9a91",
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": ".venv",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.12.4"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
diff --git a/src/bin/single_layer_classifier.rs b/src/bin/single_layer_classifier.rs
new file mode 100644
index 0000000..1258efd
--- /dev/null
+++ b/src/bin/single_layer_classifier.rs
@@ -0,0 +1,54 @@
+use std::path::PathBuf;
+
+use ndarray::Axis;
+use polars::{
+    error::PolarsError,
+    prelude::{Float64Type, IndexOrder},
+};
+use rust_ml::optim::sgd::optimizer::GradientDescent;
+use rust_ml::prelude::{Matrix, SingleLayerClassifier, Vector};
+use rust_ml::utils::data::{load_dataset, shuffle_split};
+
+type GD = GradientDescent<Matrix, Vector, SingleLayerClassifier>;
+/// In this example, we will try using the SingleLayerClassifier
+/// from the rust_ml library to classify a simple dataset.
+/// To compare the performance of the model against simple logistic regression,
+/// we will also use the diabetes dataset.
+fn main() -> Result<(), PolarsError> {
+    println!("Single Layer Classifier Example\n");
+    println!("=============================\n");
+
+    // Load the dataset
+    let dataset_path = PathBuf::from("datasets/diabetes-dataset.csv");
+    let df = load_dataset(dataset_path).unwrap();
+
+    // Print the first few rows of the dataset
+    println!("First five rows of the dataset:");
+    println!("{:?}", df.head(Some(5)));
+
+    // Extract features and target
+    let target = df.column("Outcome")?;
+    // Convert the target column to a Vec<f64> first, then to Array1
+    let target_vec: Vec<f64> = target
+        .i64()?
+        .iter()
+        .map(|opt_val| opt_val.map(|val| val as f64).unwrap_or(0.0))
+        .collect();
+    // Create an Array1 from the Vec
+    let target = ndarray::Array1::from(target_vec);
+    let features = df.drop("Outcome")?;
+
+    // Normalize the features
+    let features = features.to_ndarray::<Float64Type>(IndexOrder::C).unwrap();
+    let mean = features.mean_axis(Axis(0)).unwrap();
+    let std = features.std_axis(Axis(0), 1.0);
+    let features = (&features - &mean) / &std;
+
+    // Create train and test sets.
+    let (x_train, y_train, x_test, y_test) = shuffle_split(&features, &target, 0.8, 42);
+
+    // Initialize the optimizer
+    let gd: GD = GradientDescent::new(0.01, 1000);
+
+    Ok(())
+}
diff --git a/src/bin/sl_classifier.rs b/src/bin/sl_classifier.rs
deleted file mode 100644
index 03697de..0000000
--- a/src/bin/sl_classifier.rs
+++ /dev/null
@@ -1,14 +0,0 @@
-fn main() {
-    println!("Hello, world!");
-
-    // let model = SLClassifier()::builder()
-    // ... add props
-
-    // let optimizer = MinibatchGradientDescent(**args)
-
-    // model.fit(optimizer), or
-    // optimizer.train(model), or
-    // train(model, optimizer),
-
-    // model_performance = model.performance(x, y)
-}
diff --git a/src/builders/mod.rs b/src/builders/mod.rs
index 35b1085..5dd6d70 100644
--- a/src/builders/mod.rs
+++ b/src/builders/mod.rs
@@ -1,3 +1,4 @@
 pub mod builder;
 pub mod linear_regression;
 pub mod logistic_regression;
+pub mod single_layer_classifier;
diff --git a/src/builders/single_layer_classifier.rs b/src/builders/single_layer_classifier.rs
new file mode 100644
index 0000000..f14d344
--- /dev/null
+++ b/src/builders/single_layer_classifier.rs
@@ -0,0 +1,117 @@
+use crate::core::activations::activation_functions::ActivationFn;
+use crate::core::types::Matrix;
+use crate::prelude::{ModelError, SingleLayerClassifier};
+
+use crate::builders::builder::Builder;
+
+pub struct SingleLayerClassifierBuilder {
+    n_features: usize,
+    n_hidden_nodes: usize,
+    threshold: f64,
+    output_layer_activation: ActivationFn,
+    hidden_layer_activation: ActivationFn,
+}
+
+impl SingleLayerClassifierBuilder {
+    pub fn n_features(mut self, n_features: usize) -> Self {
+        self.n_features = n_features;
+        self
+    }
+
+    pub fn output_layer_activation_fn(mut self, activation: ActivationFn) -> Self {
+        self.output_layer_activation = activation;
+        self
+    }
+
+    pub fn hidden_layer_activation_fn(mut self, activation: ActivationFn) -> Self {
+        self.hidden_layer_activation = activation;
+        self
+    }
+
+    pub fn n_hidden_nodes(mut self, n_hidden_nodes: usize) -> Self {
+        self.n_hidden_nodes = n_hidden_nodes;
+        self
+    }
+
+    pub fn threshold(mut self, threshold: f64) -> Self {
+        self.threshold = threshold;
+        self
+    }
+}
+
+impl Default for SingleLayerClassifierBuilder {
+    fn default() -> Self {
+        Self {
+            n_features: 2,
+            n_hidden_nodes: 2,
+            threshold: 0.5,
+            output_layer_activation: ActivationFn::Sigmoid,
+            hidden_layer_activation: ActivationFn::ReLU,
+        }
+    }
+}
+
+impl Builder<SingleLayerClassifier, Matrix, Matrix> for SingleLayerClassifierBuilder {
+    fn build(&self) -> Result<SingleLayerClassifier, ModelError> {
+        Ok(SingleLayerClassifier::new(
+            self.n_features,
+            self.n_hidden_nodes,
+            self.threshold,
+            self.output_layer_activation,
+            self.hidden_layer_activation,
+        )?)
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_default_builder() {
+        let builder = SingleLayerClassifierBuilder::default();
+        assert_eq!(builder.n_features, 2);
+        assert_eq!(builder.n_hidden_nodes, 2);
+        assert_eq!(builder.threshold, 0.5);
+        match builder.output_layer_activation {
+            ActivationFn::Sigmoid => {}
+            _ => panic!("Default output activation should be Sigmoid"),
+        }
+        match builder.hidden_layer_activation {
+            ActivationFn::ReLU => {}
+            _ => panic!("Default hidden activation should be ReLU"),
+        }
+    }
+
+    #[test]
+    fn test_builder_methods() {
+        let builder = SingleLayerClassifierBuilder::default()
+            .n_features(5)
+            .n_hidden_nodes(10)
+            .output_layer_activation_fn(ActivationFn::Tanh)
+            .hidden_layer_activation_fn(ActivationFn::Sigmoid);
+
+        assert_eq!(builder.n_features, 5);
+        assert_eq!(builder.n_hidden_nodes, 10);
+        match builder.output_layer_activation {
+            ActivationFn::Tanh => {}
+            _ => panic!("Output activation should be Tanh"),
+        }
+        match builder.hidden_layer_activation {
+            ActivationFn::Sigmoid => {}
+            _ => panic!("Hidden activation should be Sigmoid"),
+        }
+    }
+
+    #[test]
+    /// Test the build method of the SingleLayerClassifierBuilder
+    fn test_builder_build() {
+        let builder = SingleLayerClassifierBuilder::default()
+            .n_features(3)
+            .n_hidden_nodes(5)
+            .output_layer_activation_fn(ActivationFn::Sigmoid)
+            .hidden_layer_activation_fn(ActivationFn::LeakyReLU);
+
+        assert!(builder.build().is_ok());
+    }
+}
diff --git a/src/core/activations/leaky_relu.rs b/src/core/activations/leaky_relu.rs
index ebeedb5..72e3289 100644
--- a/src/core/activations/leaky_relu.rs
+++ b/src/core/activations/leaky_relu.rs
@@ -1,5 +1,5 @@
 use crate::core::activations::activation::Activation;
-use ndarray::{Array, Ix1};
+use ndarray::{Array, Ix1, Ix2};
 
 pub struct LeakyReLU;
 
@@ -14,3 +14,87 @@ impl Activation<Ix1> for LeakyReLU {
         z.mapv(|x| if x > 0.0 { 1.0 } else { alpha })
     }
 }
+
+impl Activation<Ix2> for LeakyReLU {
+    fn activate(z: &Array<f64, Ix2>) -> Array<f64, Ix2> {
+        let alpha = 0.01;
+        z.mapv(|x| if x > 0.0 { x } else { alpha * x })
+    }
+
+    fn derivative(z: &Array<f64, Ix2>) -> Array<f64, Ix2> {
+        let alpha = 0.01;
+        z.mapv(|x| if x > 0.0 { 1.0 } else { alpha })
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use approx::assert_abs_diff_eq;
+    use ndarray::array;
+
+    #[test]
+    fn test_leaky_relu_activate() {
+        let input = array![2.0, -3.0, 0.0, 5.0, -1.0];
+        let expected = array![2.0, -0.03, 0.0, 5.0, -0.01];
+
+        let output = LeakyReLU::activate(&input);
+
+        assert_eq!(output.len(), expected.len());
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-10);
+        }
+    }
+
+    #[test]
+    fn test_leaky_relu_derivative() {
+        let input = array![2.0, -3.0, 0.0, 5.0, -1.0];
+        let expected = array![1.0, 0.01, 1.0, 1.0, 0.01];
+
+        let output = LeakyReLU::derivative(&input);
+
+        assert_eq!(output.len(), expected.len());
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-10);
+        }
+    }
+
+    #[test]
+    fn test_leaky_relu_zero() {
+        let input = array![0.0];
+
+        // For x = 0, LeakyReLU should return 0
+        let activate_output = LeakyReLU::activate(&input);
+        assert_abs_diff_eq!(activate_output[0], 0.0, epsilon = 1e-10);
+
+        // For x = 0, derivative should be 1.0 (treating 0 as positive)
+        let derivative_output = LeakyReLU::derivative(&input);
+        assert_abs_diff_eq!(derivative_output[0], 1.0, epsilon = 1e-10);
+    }
+
+    #[test]
+    fn test_leaky_relu_matrix_activate() {
+        let input = array![[2.0, -3.0], [0.0, -1.0]];
+        let expected = array![[2.0, -0.03], [0.0, -0.01]];
+
+        let output = LeakyReLU::activate(&input);
+
+        assert_eq!(output.shape(), expected.shape());
+        for ((i, j), val) in output.indexed_iter() {
+            assert_abs_diff_eq!(val, &expected[[i, j]], epsilon = 1e-10);
+        }
+    }
+
+    #[test]
+    fn test_leaky_relu_matrix_derivative() {
+        let input = array![[2.0, -3.0], [0.0, 5.0]];
+        let expected = array![[1.0, 0.01], [1.0, 1.0]];
+
+        let output = LeakyReLU::derivative(&input);
+
+        assert_eq!(output.shape(), expected.shape());
+        for ((i, j), val) in output.indexed_iter() {
+            assert_abs_diff_eq!(val, &expected[[i, j]], epsilon = 1e-10);
+        }
+    }
+}
diff --git a/src/core/activations/relu.rs b/src/core/activations/relu.rs
index e6cbe3e..15de030 100644
--- a/src/core/activations/relu.rs
+++ b/src/core/activations/relu.rs
@@ -1,6 +1,6 @@
-use crate::core::activations::activation::Activation;
 use crate::core::types::Vector;
-use ndarray::Ix1;
+use crate::{core::activations::activation::Activation, prelude::Matrix};
+use ndarray::{Ix1, Ix2};
 
 pub struct ReLU;
 
@@ -13,3 +13,75 @@ impl Activation<Ix1> for ReLU {
         z.map(|&x| if x > 0.0 { 1.0 } else { 0.0 })
     }
 }
+
+#[cfg(test)]
+mod tests_relu_vector {
+    use ndarray::{Array1, array};
+
+    use super::*;
+
+    #[test]
+    fn test_relu_activate_vector() {
+        let input = array![-2.0, -1.0, 0.0, 1.0, 2.0];
+        let expected = array![0.0, 0.0, 0.0, 1.0, 2.0];
+        let output = ReLU::activate(&input);
+        assert_eq!(output, expected);
+    }
+
+    #[test]
+    fn test_relu_derivative_vector() {
+        let input = array![-2.0, -1.0, 0.0, 1.0, 2.0];
+        let expected = array![0.0, 0.0, 0.0, 1.0, 1.0];
+        let output = ReLU::derivative(&input);
+        assert_eq!(output, expected);
+    }
+
+    #[test]
+    fn test_relu_activate_empty() {
+        let input: Array1<f64> = array![];
+        let expected: Array1<f64> = array![];
+        let output = ReLU::activate(&input);
+        assert_eq!(output, expected);
+    }
+}
+
+impl Activation<Ix2> for ReLU {
+    fn activate(z: &Matrix) -> Matrix {
+        z.mapv(|x| x.max(0.0))
+    }
+
+    fn derivative(z: &Matrix) -> Matrix {
+        z.map(|&x| if x > 0.0 { 1.0 } else { 0.0 })
+    }
+}
+
+#[cfg(test)]
+mod tests_relu_matrix {
+    use ndarray::{Array2, array};
+
+    use crate::core::activations::{activation::Activation, relu::ReLU};
+
+    #[test]
+    fn test_relu_activate_matrix() {
+        let input = array![[-2.0, -1.0], [0.0, 1.0], [2.0, -3.0]];
+        let expected = array![[0.0, 0.0], [0.0, 1.0], [2.0, 0.0]];
+        let output = ReLU::activate(&input);
+        assert_eq!(output, expected);
+    }
+
+    #[test]
+    fn test_relu_derivative_matrix() {
+        let input = array![[-2.0, -1.0], [0.0, 1.0], [2.0, -3.0]];
+        let expected = array![[0.0, 0.0], [0.0, 1.0], [1.0, 0.0]];
+        let output = ReLU::derivative(&input);
+        assert_eq!(output, expected);
+    }
+
+    #[test]
+    fn test_relu_activate_zeros() {
+        let input = Array2::zeros((3, 3));
+        let expected = Array2::zeros((3, 3));
+        let output = ReLU::activate(&input);
+        assert_eq!(output, expected);
+    }
+}
diff --git a/src/core/activations/tanh.rs b/src/core/activations/tanh.rs
index 980c905..e0ba100 100644
--- a/src/core/activations/tanh.rs
+++ b/src/core/activations/tanh.rs
@@ -1,15 +1,169 @@
-use crate::core::activations::activation::Activation;
 use crate::core::types::Vector;
-use ndarray::Ix1;
+use crate::{core::activations::activation::Activation, prelude::Matrix};
+use ndarray::{Ix1, Ix2};
 
 pub struct Tanh;
 
 impl Activation<Ix1> for Tanh {
     fn activate(z: &Vector) -> Vector {
-        z.mapv(|x| x.tanh())
+        (z.exp() - (-z).exp()) / (z.exp() + (-z).exp())
     }
 
     fn derivative(z: &Vector) -> Vector {
-        z.mapv(|x| 1.0 - x.tanh().powi(2))
+        let a = (z.exp() - (-z).exp()) / (z.exp() + (-z).exp());
+        1.0 - a.powi(2)
+    }
+}
+
+#[cfg(test)]
+mod tests_tanh_vector {
+    use super::*;
+    use approx::assert_abs_diff_eq;
+    use ndarray::array;
+
+    #[test]
+    fn test_tanh_activate() {
+        let input = array![0.0, 1.0, -1.0, 2.0, -2.0];
+        let expected = array![
+            0.0,
+            0.7615941559557649,
+            -0.7615941559557649,
+            0.9640275800758169,
+            -0.9640275800758169
+        ];
+
+        let output = Tanh::activate(&input);
+
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-10);
+        }
+    }
+
+    #[test]
+    fn test_tanh_derivative() {
+        let input = array![0.0, 1.0, -1.0, 2.0, -2.0];
+        let expected = array![
+            1.0,
+            0.41997434161402614,
+            0.41997434161402614,
+            0.07065082485316443,
+            0.07065082485316443
+        ];
+
+        let output = Tanh::derivative(&input);
+
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-10);
+        }
+    }
+
+    #[test]
+    fn test_tanh_activate_large_values() {
+        let input = array![10.0, -10.0, 100.0, -100.0];
+        let expected = array![0.9999999958776927, -0.9999999958776927, 1.0, -1.0];
+
+        let output = Tanh::activate(&input);
+
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-8);
+        }
+    }
+
+    #[test]
+    fn test_tanh_derivative_large_values() {
+        let input = array![10.0, -10.0, 100.0, -100.0];
+        let expected = array![8.24461455076283e-9, 8.24461455076283e-9, 0.0, 0.0];
+
+        let output = Tanh::derivative(&input);
+
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-8);
+        }
+    }
+
+    #[test]
+    fn test_tanh_activate_empty() {
+        let input: Vector = array![];
+        let output = Tanh::activate(&input);
+        assert_eq!(output.len(), 0);
+    }
+}
+
+impl Activation<Ix2> for Tanh {
+    fn activate(z: &Matrix) -> Matrix {
+        (z.exp() - (-z).exp()) / (z.exp() + (-z).exp())
+    }
+
+    fn derivative(z: &Matrix) -> Matrix {
+        let a = (z.exp() - (-z).exp()) / (z.exp() + (-z).exp());
+        1.0 - a.powi(2)
+    }
+}
+#[cfg(test)]
+mod tests_tanh_matrix {
+    use approx::assert_abs_diff_eq;
+    use ndarray::{Array2, array};
+
+    use super::*;
+
+    #[test]
+    fn test_tanh_activate_matrix() {
+        let input = array![[0.0, 1.0], [-1.0, 2.0]];
+        let expected = array![
+            [0.0, 0.7615941559557649],
+            [-0.7615941559557649, 0.9640275800758169]
+        ];
+
+        let output = Tanh::activate(&input);
+
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-10);
+        }
+    }
+
+    #[test]
+    fn test_tanh_derivative_matrix() {
+        let input = array![[0.0, 1.0], [-1.0, 2.0]];
+        let expected = array![
+            [1.0, 0.41997434161402614],
+            [0.41997434161402614, 0.07065082485316443]
+        ];
+
+        let output = Tanh::derivative(&input);
+
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-10);
+        }
+    }
+
+    #[test]
+    fn test_tanh_activate_large_values_matrix() {
+        let input = array![[10.0, -10.0], [100.0, -100.0]];
+        let expected = array![[0.9999999958776927, -0.9999999958776927], [1.0, -1.0]];
+
+        let output = Tanh::activate(&input);
+
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-8);
+        }
+    }
+
+    #[test]
+    fn test_tanh_derivative_large_values_matrix() {
+        let input = array![[10.0, -10.0], [100.0, -100.0]];
+        let expected = array![[8.24461455076283e-9, 8.24461455076283e-9], [0.0, 0.0]];
+
+        let output = Tanh::derivative(&input);
+
+        for (a, b) in output.iter().zip(expected.iter()) {
+            assert_abs_diff_eq!(a, b, epsilon = 1e-8);
+        }
+    }
+
+    #[test]
+    fn test_tanh_activate_empty_matrix() {
+        let input: Array2<f64> = Array2::zeros((0, 0));
+        let output = Tanh::activate(&input);
+        assert_eq!(output.shape(), &[0, 0]);
     }
 }
diff --git a/src/core/types.rs b/src/core/types.rs
index 2bc34ed..741da9f 100644
--- a/src/core/types.rs
+++ b/src/core/types.rs
@@ -1,7 +1,10 @@
-use ndarray::{Array, Array0, Array1, Array2, IxDyn};
+use ndarray::{Array, Array0, Array1, Array2, ArrayView2, IxDyn};
 use std::collections::HashMap;
 
 pub type ParamsHashmap = HashMap<String, Array<f64, IxDyn>>;
 pub type Matrix = Array2<f64>;
+pub type MatrixView<'a> = ArrayView2<'a, f64>;
 pub type Vector = Array1<f64>;
+pub type VectorView = ArrayView2<'static, f64>;
 pub type Scalar = Array0<f64>;
+pub type ScalarView = ArrayView2<'static, f64>;
diff --git a/src/lib.rs b/src/lib.rs
index 59ed685..cbc9b57 100644
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -4,3 +4,11 @@ pub mod core;
 pub mod model;
 pub mod optim;
 pub mod utils;
+
+/// The prelude module provides convenient imports of common components.
+///
+/// Users can import all commonly used types at once with:
+/// ```rust
+/// use rust_ml::prelude::*;
+/// ```
+pub mod prelude;
diff --git a/src/model/logistic_regression.rs b/src/model/logistic_regression.rs
index 916593c..0fec196 100644
--- a/src/model/logistic_regression.rs
+++ b/src/model/logistic_regression.rs
@@ -88,7 +88,7 @@ impl LogisticRegression {
         }
     }
 
-    fn compute_z(&self, x: &Matrix) -> Result<Vector, ModelError> {
+    fn compute_linear_activation(&self, x: &Matrix) -> Result<Vector, ModelError> {
         let z = self.weights.t().dot(x) + &self.bias;
         Ok(z)
     }
@@ -181,7 +181,7 @@ impl GradientCollection for LogisticRegression {
 
 impl OptimizableModel<Matrix, Vector> for LogisticRegression {
     fn forward(&self, input: &Matrix) -> Result<Vector, ModelError> {
-        let z = self.compute_z(input)?;
+        let z = self.compute_linear_activation(input)?;
         let a = self.compute_activation(&z)?;
         // Make activation numerically safer
         let epsilon = 1e-15;
@@ -219,7 +219,7 @@ impl OptimizableModel<Matrix, Vector> for LogisticRegression {
     /// The gradient vector
     fn compute_output_gradient(&self, x: &Matrix, y: &Vector) -> Result<Vector, ModelError> {
         // Forward pass to get predictions
-        let z = self.compute_z(x)?;
+        let z = self.compute_linear_activation(x)?;
         let y_hat = self.compute_activation(&z)?;
 
         // Compute the derivative of the activation function
diff --git a/src/model/mod.rs b/src/model/mod.rs
index c4789d8..d6ca8dc 100644
--- a/src/model/mod.rs
+++ b/src/model/mod.rs
@@ -1,4 +1,4 @@
 pub mod core;
 pub mod linear_regression;
 pub mod logistic_regression;
-// pub mod logistic_regression;
+pub mod single_layer_classifier;
diff --git a/src/model/single_layer_classifier.rs b/src/model/single_layer_classifier.rs
new file mode 100644
index 0000000..faf4ebd
--- /dev/null
+++ b/src/model/single_layer_classifier.rs
@@ -0,0 +1,872 @@
+use crate::core::activations::activation::Activation;
+use crate::core::activations::leaky_relu::LeakyReLU;
+use crate::core::activations::relu::ReLU;
+use crate::core::activations::sigmoid::Sigmoid;
+use crate::core::activations::tanh::Tanh;
+use crate::model::core::base::{BaseModel, OptimizableModel};
+use crate::model::core::param_collection::{GradientCollection, ParamCollection};
+use crate::prelude::single_layer_classifier::SingleLayerClassifierBuilder;
+use crate::prelude::*;
+use ndarray::{ArrayView, Dimension};
+use ndarray_rand::RandomExt;
+use ndarray_rand::rand_distr::Normal;
+
+/// Single hidden layer classifier model.
+/// This model is a simple feedforward neural network with one hidden layer.
+///
+/// # Structure
+/// This neural network consists of:
+/// * An input layer with n_features nodes
+/// * A single hidden layer with n_hidden_nodes nodes using a configurable activation function
+/// * An output layer with a single node using a configurable activation function
+///
+/// # Parameters
+/// * `w1` - Weight matrix between input and hidden layer (size: n_hidden_nodes × n_features)
+/// * `b1` - Bias vector for hidden layer (size: n_hidden_nodes)
+/// * `w2` - Weight matrix between hidden and output layer (size: 1 × n_hidden_nodes)
+/// * `b2` - Bias vector for output layer (size: n_hidden_nodes)
+/// * `output_layer_activation_fn` - Activation function applied to the output layer
+/// * `hidden_layer_activation_fn` - Activation function applied to the hidden layer
+/// * `threshold` - Classification threshold for binary classification
+#[derive(Debug, Clone)]
+pub struct SingleLayerClassifier {
+    pub w1: Matrix,
+    pub b1: Vector,
+    pub w2: Matrix,
+    pub b2: Vector,
+    output_layer_activation_fn: ActivationFn,
+    hidden_layer_activation_fn: ActivationFn,
+    threshold: f64,
+}
+
+impl SingleLayerClassifier {
+    /// Creates a new single layer neural network classifier.
+    ///
+    /// # Parameters
+    /// * `n_features` - The number of input features.
+    /// * `n_hidden_nodes` - The number of nodes in the hidden layer.
+    /// * `threshold` - Classification threshold between 0.0 and 1.0.
+    ///                Outputs above this value are classified as positive.
+    /// * `output_layer_activation_fn` - Activation function for the output layer.
+    /// * `hidden_layer_activation_fn` - Activation function for the hidden layer.
+    ///
+    /// # Returns
+    /// * `Result<SingleLayerClassifier, ModelError>` - A new classifier instance or an error.
+    ///
+    /// # Errors
+    /// Returns `ModelError::InvalidParameter` if the threshold is not between 0.0 and 1.0.
+    ///
+    /// # Details
+    /// Initializes the weights using a standard normal distribution and biases with zeros.
+    ///
+    /// # Example
+    /// ```
+    /// use rust_ml::prelude::*;
+    /// let classifier = SingleLayerClassifier::new(
+    ///     4, // features
+    ///     10, // hidden nodes
+    ///     0.5, // threshold
+    ///     ActivationFn::Sigmoid,
+    ///     ActivationFn::ReLU
+    /// )?;
+    /// ```
+    ///
+    /// Alternatively, use the builder pattern:
+    /// ```
+    /// use rust_ml::builders::builder::Builder;
+    /// use rust_ml::prelude::SingleLayerClassifier;
+    /// let classifier = SingleLayerClassifier::builder()
+    ///     // configure with builder methods
+    ///     .build()?;
+    /// ```
+    pub fn new(
+        n_features: usize,
+        n_hidden_nodes: usize,
+        threshold: f64,
+        output_layer_activation_fn: ActivationFn,
+        hidden_layer_activation_fn: ActivationFn,
+    ) -> Result<SingleLayerClassifier, ModelError> {
+        // Check that the threshold is between 0.0 and 1.0
+        if !(0.0..=1.0).contains(&threshold) {
+            return Err(ModelError::InvalidParameter(
+                "Threshold must be between 0.0 and 1.0".to_string(),
+            ));
+        }
+
+        // Initialize weights and biases using a normal distribution for weights, and zeros for biases
+        let distribution = Normal::new(0.0, 1.0).unwrap();
+        let w1 = Matrix::random((n_hidden_nodes, n_features), distribution);
+        let b1 = Vector::zeros(n_hidden_nodes);
+        let w2 = Matrix::random((1, n_hidden_nodes), distribution);
+        let b2 = Vector::zeros(1);
+
+        Ok(Self {
+            w1,
+            b1,
+            w2,
+            b2,
+            output_layer_activation_fn,
+            hidden_layer_activation_fn,
+            threshold,
+        })
+    }
+
+    /// Creates a builder for configuring a new SingleLayerClassifier.
+    ///
+    /// This method returns a builder that allows for a fluent interface
+    /// to configure and construct a new classifier instance.
+    ///
+    /// # Returns
+    /// A new SingleLayerClassifierBuilder instance with default settings.
+    ///
+    /// # Example
+    /// ```
+    /// use rust_ml::builders::builder::Builder;
+    /// use rust_ml::prelude::SingleLayerClassifier;
+    ///
+    /// let classifier = SingleLayerClassifier::builder()
+    ///     .n_features(4)
+    ///     .n_hidden_nodes(10)
+    ///     .threshold(0.5)
+    ///     .build()?;
+    /// ```
+    pub fn builder() -> SingleLayerClassifierBuilder {
+        SingleLayerClassifierBuilder::default()
+    }
+
+    /// Returns the activation for a given input z.
+    pub fn compute_activation(&self, z: &Matrix, activation_fn: ActivationFn) -> Matrix {
+        match activation_fn {
+            ActivationFn::Sigmoid => Sigmoid::activate(z),
+            ActivationFn::ReLU => ReLU::activate(z),
+            ActivationFn::Tanh => Tanh::activate(z),
+            ActivationFn::LeakyReLU => LeakyReLU::activate(z),
+        }
+    }
+
+    /// Computes the linear activation for a given input x, weights w, and bias b.
+    /// Given the following dimensions:
+    /// * x: (n_features, m)
+    /// * w: (n_nodes, n_features)
+    /// * b: (n_nodes)
+    ///
+    /// The dimensions of the output will be:
+    ///
+    /// * (n_nodes, m)
+    pub fn compute_linear_activation(
+        &self,
+        x: &Matrix,
+        w: &Matrix,
+        b: &Vector,
+    ) -> Result<Matrix, ModelError> {
+        let m = x.shape()[1];
+        let b = b.to_shape((b.len(), 1))?;
+        let n_nodes = w.shape()[0];
+        let b = b
+            .broadcast((n_nodes, m))
+            .ok_or(ModelError::ShapeError("Broadcasting failed".to_string()))?;
+        let z = w.dot(x) + b;
+        Ok(z)
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::builders::builder::Builder;
+
+    #[test]
+    fn test_new_valid_parameters() {
+        let result =
+            SingleLayerClassifier::new(4, 10, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU);
+
+        assert!(result.is_ok());
+        let classifier = result.unwrap();
+
+        assert_eq!(classifier.w1.shape(), &[10, 4]);
+        assert_eq!(classifier.b1.len(), 10);
+        assert_eq!(classifier.w2.shape(), &[1, 10]);
+        assert_eq!(classifier.b2.len(), 10);
+        assert_eq!(classifier.threshold, 0.5);
+    }
+
+    #[test]
+    fn test_new_invalid_threshold() {
+        let result =
+            SingleLayerClassifier::new(4, 10, 1.5, ActivationFn::Sigmoid, ActivationFn::ReLU);
+
+        assert!(result.is_err());
+        match result {
+            Err(ModelError::InvalidParameter(msg)) => {
+                assert!(msg.contains("Threshold must be between 0.0 and 1.0"));
+            }
+            _ => panic!("Expected InvalidParameter error"),
+        }
+    }
+
+    #[test]
+    fn test_builder_pattern() {
+        let result = SingleLayerClassifier::builder()
+            .n_features(4)
+            .n_hidden_nodes(10)
+            .threshold(0.5)
+            .output_layer_activation_fn(ActivationFn::Sigmoid)
+            .output_layer_activation_fn(ActivationFn::ReLU)
+            .build();
+
+        assert!(result.is_ok());
+
+        let classifier = result.unwrap();
+
+        assert_eq!(classifier.w1.shape(), &[10, 4]);
+        assert_eq!(classifier.b1.len(), 10);
+        assert_eq!(classifier.w2.shape(), &[1, 10]);
+        assert_eq!(classifier.b2.len(), 10);
+    }
+
+    /// Test the compute_activation method
+    #[test]
+    fn test_compute_activation() {
+        let classifier =
+            SingleLayerClassifier::new(4, 10, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        let input = Matrix::from_shape_vec((10, 1), vec![0.1; 10]).unwrap();
+        let result = classifier.compute_activation(&input, ActivationFn::Sigmoid);
+        assert_eq!(result.shape(), &[1, 1]);
+
+        let result = classifier.compute_activation(&input, ActivationFn::ReLU);
+        assert_eq!(result.shape(), &[1, 1]);
+    }
+
+    /// Test the compute_linear_activation method
+    #[test]
+    fn test_compute_linear_activation() {
+        let classifier =
+            SingleLayerClassifier::new(4, 10, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        let x = Matrix::from_shape_vec((4, 2), vec![0.1; 8]).unwrap();
+        let result = classifier.compute_linear_activation(&x, &classifier.w1, &classifier.b1);
+        assert!(result.is_ok());
+        let output = result.unwrap();
+        assert_eq!(output.shape(), &[10, 2]);
+    }
+}
+
+impl BaseModel<Matrix, Matrix> for SingleLayerClassifier {
+    /// Predicts output values for given input features.
+    ///
+    /// This method performs a forward pass through the neural network to generate predictions.
+    ///
+    /// # Parameters
+    /// * `x` - Input feature matrix where each columns represents a sample
+    ///
+    /// # Returns
+    /// * `Result<Vector, ModelError>` - Vector of prediction values (before thresholding) or an error
+    ///
+    /// # Errors
+    /// Returns `ModelError` if matrix dimensions are incompatible
+    fn predict(&self, x: &Matrix) -> Result<Matrix, ModelError> {
+        let a2 = self.forward(x)?;
+        let y_hat = a2.map(|v| if v > &self.threshold { 1.0 } else { 0.0 });
+        Ok(y_hat)
+    }
+
+    /// Computes the loss/cost for predictions against actual values.
+    ///
+    /// This method calculates the binary cross-entropy loss for binary classification,
+    /// which measures how well the model's predictions match the true labels.
+    ///
+    /// # Parameters
+    /// * `x` - Input feature matrix where each column represents a sample
+    /// * `y` - Target vector with ground truth labels (0 or 1)
+    ///
+    /// # Returns
+    /// * `Result<f64, ModelError>` - The average loss across all samples or an error
+    ///
+    /// # Errors
+    /// Returns `ModelError` if dimensions are incompatible or if the forward pass fails
+    fn compute_cost(&self, x: &Matrix, y: &Matrix) -> Result<f64, ModelError> {
+        // Perform forward pass to get predictions
+        let predictions = self.forward(x)?;
+
+        // Ensure y and predictions have compatible dimensions
+        if y.shape() != predictions.shape() {
+            return Err(ModelError::ShapeError(format!(
+                "Target shape {:?} does not match predictions shape {:?}",
+                y.shape(),
+                predictions.shape()
+            )));
+        }
+
+        // Compute binary cross-entropy loss
+        // L = -1/m * sum(y * log(a) + (1-y) * log(1-a))
+        let m = y.len() as f64;
+        let epsilon = 1e-15; // Small constant to avoid log(0)
+
+        // Clip predictions to avoid numerical instability
+        let predictions = predictions.mapv(|a| a.max(epsilon).min(1.0 - epsilon));
+
+        // Calculate element-wise terms of the loss function
+        let term1 = y * &predictions.mapv(|a| a.ln());
+        let term2 = (1.0 - y) * &predictions.mapv(|a| (1.0 - a).ln());
+
+        // Calculate total loss
+        let total_loss = -1.0 * (term1 + term2).sum() / m;
+
+        Ok(total_loss)
+    }
+}
+
+#[cfg(test)]
+mod base_model_tests {
+    use super::*;
+    use approx::assert_relative_eq;
+    use ndarray::arr2;
+
+    #[test]
+    fn test_predict() {
+        // Create a classifier with known parameters
+        let mut classifier = SingleLayerClassifier::new(
+            2,   // 2 features
+            3,   // 3 hidden nodes
+            0.5, // threshold
+            ActivationFn::Sigmoid,
+            ActivationFn::ReLU,
+        )
+        .unwrap();
+
+        // Set predetermined weights to get deterministic outputs
+        classifier.w1 = Matrix::from_shape_vec((3, 2), vec![0.1, 0.2, 0.3, 0.4, 5.0, 6.0]).unwrap();
+        classifier.b1 = Vector::from_vec(vec![0.1, 0.2, 0.3]);
+        classifier.w2 = Matrix::from_shape_vec((1, 3), vec![0.1, 0.2, 0.3]).unwrap();
+        classifier.b2 = Vector::from_vec(vec![0.1]);
+
+        // Input data with 2 samples
+        let input = Matrix::from_shape_vec((2, 2), vec![1.0, 2.0, 3.0, 4.0]).unwrap();
+
+        // Expected probability output: [0.65432483, 0.65889896]
+        let expected_predictions = arr2(&[[1.0, 1.0]]);
+
+        let result = classifier.predict(&input);
+        assert!(result.is_ok());
+
+        // Get the predictions
+        let predictions = result.unwrap();
+        assert_eq!(predictions, expected_predictions);
+
+        // Check the shape of the predictions
+        assert_eq!(predictions.shape(), &[1, 2]);
+        // All predictions should be either 0.0 or 1.0 (binary classification)
+        for val in predictions.iter() {
+            assert!(*val == 0.0 || *val == 1.0);
+        }
+    }
+
+    #[test]
+    fn test_predict_with_threshold() {
+        // Create two classifiers with different thresholds
+        let mut classifier_low_threshold =
+            SingleLayerClassifier::new(2, 3, 0.3, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        let mut classifier_high_threshold =
+            SingleLayerClassifier::new(2, 3, 0.7, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        // Set identical weights for both classifiers
+        let w1 = Matrix::from_shape_vec((3, 2), vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6]).unwrap();
+        let b1 = Vector::from_vec(vec![0.1, 0.2, 0.3]);
+        let w2 = Matrix::from_shape_vec((1, 3), vec![0.7, 0.8, 0.9]).unwrap();
+        let b2 = Vector::from_vec(vec![0.1]);
+
+        classifier_low_threshold.w1 = w1.clone();
+        classifier_low_threshold.b1 = b1.clone();
+        classifier_low_threshold.w2 = w2.clone();
+        classifier_low_threshold.b2 = b2.clone();
+
+        classifier_high_threshold.w1 = w1;
+        classifier_high_threshold.b1 = b1;
+        classifier_high_threshold.w2 = w2;
+        classifier_high_threshold.b2 = b2;
+
+        // Input data
+        let input = Matrix::from_shape_vec((2, 1), vec![1.0, 2.0]).unwrap();
+
+        // The low threshold classifier should be more likely to predict 1s
+        let pred_low = classifier_low_threshold.predict(&input).unwrap();
+        let pred_high = classifier_high_threshold.predict(&input).unwrap();
+
+        // For this particular input and weights, we expect different predictions
+        // based on the threshold (this is based on the sigmoid output being between 0.3 and 0.7)
+        // This is less brittle than checking for specific values
+        assert!(pred_low.sum() >= pred_high.sum());
+    }
+
+    #[test]
+    fn test_compute_cost() {
+        // Create a classifier
+        let classifier =
+            SingleLayerClassifier::new(2, 3, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        // Create simple test data
+        let x = Matrix::from_shape_vec((2, 3), vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6]).unwrap();
+
+        // For perfect predictions, cost should be close to 0
+        let perfect_preds = Matrix::from_shape_vec((1, 3), vec![1.0, 0.0, 1.0]).unwrap();
+        let perfect_y = Matrix::from_shape_vec((1, 3), vec![1.0, 0.0, 1.0]).unwrap();
+
+        let cost_perfect = classifier.compute_cost(&x, &perfect_y).unwrap();
+        assert_relative_eq!(cost_perfect, 0.0, epsilon = 1e-10);
+
+        // For completely wrong predictions, cost should be high
+        let wrong_preds = Matrix::from_shape_vec((1, 3), vec![0.01, 0.99, 0.01]).unwrap();
+        let wrong_y = Matrix::from_shape_vec((1, 3), vec![0.99, 0.01, 0.99]).unwrap();
+
+        let cost_wrong = classifier.compute_cost(&x, &wrong_y).unwrap();
+        assert!(cost_wrong > 1.0); // Binary cross-entropy should be high for wrong predictions
+    }
+
+    #[test]
+    fn test_compute_cost_validation() {
+        let classifier =
+            SingleLayerClassifier::new(2, 3, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        // Input data
+        let x = Matrix::from_shape_vec((2, 2), vec![0.1, 0.2, 0.3, 0.4]).unwrap();
+
+        // Target with incorrect shape
+        let y_wrong_shape = Matrix::from_shape_vec((2, 2), vec![1.0, 0.0, 1.0, 0.0]).unwrap();
+
+        // The compute_cost function should return an error due to shape mismatch
+        let result = classifier.compute_cost(&x, &y_wrong_shape);
+        assert!(result.is_err());
+
+        if let Err(ModelError::ShapeError(_)) = result {
+            // Good, we got the expected error type
+        } else {
+            panic!("Expected ShapeError, got {:?}", result);
+        }
+    }
+}
+
+impl ParamCollection for SingleLayerClassifier {
+    /// Retrieves a read-only view of the specified parameter.
+    ///
+    /// This method provides access to the model's parameters by name.
+    ///
+    /// # Parameters
+    /// * `key` - The name of the parameter to retrieve: "w1", "b1", "w2", or "b2"
+    ///
+    /// # Returns
+    /// * `Result<ndarray::ArrayView<f64, D>, ModelError>` - A view of the parameter or an error
+    ///
+    /// # Errors
+    /// Returns `ModelError::InvalidParameter` if the key doesn't match any parameter,
+    /// or `ShapeError` if the dimension conversion fails.
+    fn get<D: ndarray::Dimension>(
+        &self,
+        key: &str,
+    ) -> Result<ndarray::ArrayView<f64, D>, ModelError> {
+        match key {
+            "w1" => Ok(self.w1.view().into_dimensionality::<D>()?),
+            "b1" => Ok(self.b1.view().into_dimensionality::<D>()?),
+            "w2" => Ok(self.w2.view().into_dimensionality::<D>()?),
+            "b2" => Ok(self.b2.view().into_dimensionality::<D>()?),
+            _ => Err(ModelError::InvalidParameter(format!(
+                "Invalid parameter key: {}",
+                key
+            ))),
+        }
+    }
+
+    /// Retrieves a mutable view of the specified parameter.
+    ///
+    /// This method provides mutable access to the model's parameters by name,
+    /// allowing for parameter updates during training.
+    ///
+    /// # Parameters
+    /// * `key` - The name of the parameter to retrieve: "w1", "b1", "w2", or "b2"
+    ///
+    /// # Returns
+    /// * `Result<ndarray::ArrayViewMut<f64, D>, ModelError>` - A mutable view of the parameter or an error
+    ///
+    /// # Errors
+    /// Returns `ModelError::InvalidParameter` if the key doesn't match any parameter,
+    /// or `ShapeError` if the dimension conversion fails.
+    fn get_mut<D: ndarray::Dimension>(
+        &mut self,
+        key: &str,
+    ) -> Result<ndarray::ArrayViewMut<f64, D>, ModelError> {
+        match key {
+            "w1" => Ok(self.w1.view_mut().into_dimensionality::<D>()?),
+            "b1" => Ok(self.b1.view_mut().into_dimensionality::<D>()?),
+            "w2" => Ok(self.w2.view_mut().into_dimensionality::<D>()?),
+            "b2" => Ok(self.b2.view_mut().into_dimensionality::<D>()?),
+            _ => Err(ModelError::InvalidParameter(format!(
+                "Invalid parameter key: {}",
+                key
+            ))),
+        }
+    }
+
+    /// Sets a parameter to the provided value.
+    ///
+    /// This method updates a parameter with new values, performing shape validation.
+    ///
+    /// # Parameters
+    /// * `key` - The name of the parameter to update: "w1", "b1", "w2", or "b2"
+    /// * `value` - The new values to assign to the parameter
+    ///
+    /// # Returns
+    /// * `Result<(), ModelError>` - Success or an error
+    ///
+    /// # Errors
+    /// Returns `ModelError::InvalidParameter` if the key doesn't match any parameter,
+    /// or `ShapeError` if the shapes are incompatible.
+    fn set<D: ndarray::Dimension>(
+        &mut self,
+        key: &str,
+        value: ndarray::ArrayView<f64, D>,
+    ) -> Result<(), ModelError> {
+        match key {
+            "w1" => {
+                self.w1.assign(&value.to_shape(self.w1.shape())?);
+                Ok(())
+            }
+            "b1" => {
+                self.b1.assign(&value.to_shape(self.b1.shape())?);
+                Ok(())
+            }
+            "w2" => {
+                self.w2.assign(&value.to_shape(self.w2.shape())?);
+                Ok(())
+            }
+            "b2" => {
+                self.b2.assign(&value.to_shape(self.b2.shape())?);
+                Ok(())
+            }
+            _ => Err(ModelError::InvalidParameter(format!(
+                "Invalid parameter key: {}",
+                key
+            ))),
+        }
+    }
+
+    /// Returns an iterator over all parameters.
+    ///
+    /// # Returns
+    /// A vector of tuples containing parameter names and their corresponding array views.
+    fn param_iter(&self) -> Vec<(&str, ndarray::ArrayView<f64, ndarray::IxDyn>)> {
+        vec![
+            ("w1", self.w1.view().into_dyn()),
+            ("b1", self.b1.view().into_dyn()),
+            ("w2", self.w2.view().into_dyn()),
+            ("b2", self.b2.view().into_dyn()),
+        ]
+    }
+}
+
+#[cfg(test)]
+mod param_collection_tests {
+    use crate::{
+        model::core::param_collection::ParamCollection,
+        prelude::{ActivationFn, Matrix, SingleLayerClassifier},
+    };
+
+    #[test]
+    fn test_param_collection_get() {
+        let classifier =
+            SingleLayerClassifier::new(4, 10, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        let w1 = classifier.get::<ndarray::Ix2>("w1");
+        assert!(w1.is_ok());
+        assert_eq!(w1.unwrap().shape(), &[10, 4]);
+
+        let invalid = classifier.get::<ndarray::Ix2>("invalid");
+        assert!(invalid.is_err());
+    }
+
+    #[test]
+    fn test_param_collection_set() {
+        let mut classifier =
+            SingleLayerClassifier::new(4, 10, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        let new_w1 = Matrix::zeros((10, 4));
+        let result = classifier.set("w1", new_w1.view());
+        assert!(result.is_ok());
+
+        let wrong_shape = Matrix::zeros((5, 5));
+        let result = classifier.set("w1", wrong_shape.view());
+        assert!(result.is_err());
+
+        let invalid = classifier.set("invalid", new_w1.view());
+        assert!(invalid.is_err());
+    }
+
+    #[test]
+    fn test_param_iter() {
+        let classifier =
+            SingleLayerClassifier::new(4, 10, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        let params = classifier.param_iter();
+        assert_eq!(params.len(), 4);
+        assert_eq!(params[0].0, "w1");
+        assert_eq!(params[1].0, "b1");
+        assert_eq!(params[2].0, "w2");
+        assert_eq!(params[3].0, "b2");
+    }
+}
+
+impl GradientCollection for SingleLayerClassifier {
+    fn get_gradient<D: Dimension>(&self, key: &str) -> Result<ArrayView<f64, D>, ModelError> {
+        match key {
+            "w1" => Ok(self.w1.view().into_dimensionality::<D>()?),
+            "b1" => Ok(self.b1.view().into_dimensionality::<D>()?),
+            "w2" => Ok(self.w2.view().into_dimensionality::<D>()?),
+            "b2" => Ok(self.b2.view().into_dimensionality::<D>()?),
+            _ => Err(ModelError::InvalidParameter(format!(
+                "Invalid parameter key: {}",
+                key
+            ))),
+        }
+    }
+
+    fn set_gradient<D: Dimension>(
+        &mut self,
+        key: &str,
+        value: ArrayView<f64, D>,
+    ) -> Result<(), ModelError> {
+        match key {
+            "w1" => {
+                self.w1.assign(&value.to_shape(self.w1.shape())?);
+                Ok(())
+            }
+            "b1" => {
+                self.b1.assign(&value.to_shape(self.b1.shape())?);
+                Ok(())
+            }
+            "w2" => {
+                self.w2.assign(&value.to_shape(self.w2.shape())?);
+                Ok(())
+            }
+            "b2" => {
+                self.b2.assign(&value.to_shape(self.b2.shape())?);
+                Ok(())
+            }
+            _ => Err(ModelError::InvalidParameter(format!(
+                "Invalid parameter key: {}",
+                key
+            ))),
+        }
+    }
+}
+#[cfg(test)]
+mod gradient_collection_tests {
+    use ndarray::Array;
+
+    use crate::{
+        model::core::param_collection::GradientCollection,
+        prelude::{ActivationFn, Matrix, SingleLayerClassifier, Vector},
+    };
+
+    #[test]
+    fn test_gradient_collection_get() {
+        let classifier =
+            SingleLayerClassifier::new(4, 10, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        let w1_grad = classifier.get_gradient::<ndarray::Ix2>("w1");
+        assert!(w1_grad.is_ok());
+        assert_eq!(w1_grad.unwrap().shape(), &[10, 4]);
+
+        let b1_grad = classifier.get_gradient::<ndarray::Ix1>("b1");
+        assert!(b1_grad.is_ok());
+        assert_eq!(b1_grad.unwrap().shape(), &[10]);
+
+        let invalid = classifier.get_gradient::<ndarray::Ix2>("invalid");
+        assert!(invalid.is_err());
+    }
+
+    #[test]
+    fn test_gradient_collection_set() {
+        let mut classifier =
+            SingleLayerClassifier::new(4, 10, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        let new_w1_grad = Matrix::zeros((10, 4));
+        let result = classifier.set_gradient("w1", new_w1_grad.view());
+        assert!(result.is_ok());
+
+        let new_b1_grad = Vector::ones(10);
+        let result = classifier.set_gradient("b1", new_b1_grad.view());
+        assert!(result.is_ok());
+
+        // Test with wrong shape
+        let wrong_shape = Matrix::zeros((5, 5));
+        let result = classifier.set_gradient("w1", wrong_shape.view());
+        assert!(result.is_err());
+
+        // Test with invalid key
+        let invalid = classifier.set_gradient("invalid", new_w1_grad.view());
+        assert!(invalid.is_err());
+    }
+
+    #[test]
+    fn test_gradient_dimension_conversion() {
+        let mut classifier =
+            SingleLayerClassifier::new(4, 10, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        // Test getting with dynamic dimensions
+        let w2_grad = classifier.get_gradient::<ndarray::IxDyn>("w2");
+        assert!(w2_grad.is_ok());
+        assert_eq!(w2_grad.unwrap().shape(), &[1, 10]);
+
+        // Test setting with dynamic dimensions
+        let new_b2_grad = Array::ones(ndarray::IxDyn(&[10]));
+        let result = classifier.set_gradient("b2", new_b2_grad.view());
+        assert!(result.is_ok());
+    }
+}
+
+impl OptimizableModel<Matrix, Matrix> for SingleLayerClassifier {
+    fn forward(&self, input: &Matrix) -> Result<Matrix, ModelError> {
+        // Forward pass through the network
+        // Shape of input: (n_features, n_samples)
+        // Shape of w1: (n_hidden_nodes, n_features)
+        // Shape of b1: (n_hidden_nodes)
+        // Shape of z1: (n_hidden_nodes, n_samples)
+        // z1 = w1 @ input + b1
+        // (n_hidden_nodes, n_samples) = (n_hidden_nodes, n_features) @ (n_features, n_samples) + (n_hidden_nodes)
+        // Extract the number of samples, and the number of input features
+        let z1 = self.compute_linear_activation(input, &self.w1, &self.b1)?;
+        let a1 = self.compute_activation(&z1, self.hidden_layer_activation_fn);
+        let z2 = self.compute_linear_activation(&a1, &self.w2, &self.b2)?;
+        let a2 = self.compute_activation(&z2, self.output_layer_activation_fn);
+        // a2 is the output of the model
+        Ok(a2)
+    }
+
+    fn backward(&mut self, input: &Matrix, output_grad: &Matrix) -> Result<(), ModelError> {
+        todo!()
+    }
+
+    fn compute_output_gradient(&self, x: &Matrix, y: &Matrix) -> Result<Matrix, ModelError> {
+        todo!()
+    }
+}
+#[cfg(test)]
+mod optimizable_model_tests {
+    use super::*;
+    use approx::assert_relative_eq;
+
+    #[test]
+    fn test_forward_dimensions() {
+        let classifier = SingleLayerClassifier::new(
+            3, // 3 features
+            5, // 5 hidden nodes
+            0.5,
+            ActivationFn::Sigmoid,
+            ActivationFn::ReLU,
+        )
+        .unwrap();
+
+        // Input with 3 features and 2 samples
+        let input = Matrix::from_shape_vec((3, 2), vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6]).unwrap();
+
+        let result = classifier.forward(&input);
+        assert!(result.is_ok());
+
+        let output = result.unwrap();
+        // Should return one value per sample
+        assert_eq!(output.len(), 2);
+        assert_eq!(output.shape(), &[1, 2]);
+    }
+
+    #[test]
+    fn test_forward_with_relu_sigmoid() {
+        // Create a model with controlled weights for deterministic testing
+        let mut classifier = SingleLayerClassifier::new(
+            2, // 2 features
+            3, // 3 hidden nodes
+            0.5,
+            ActivationFn::Sigmoid,
+            ActivationFn::ReLU,
+        )
+        .unwrap();
+
+        // Set predetermined weights and biases
+        classifier.w1 = Matrix::from_shape_vec((3, 2), vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6]).unwrap();
+        classifier.b1 = Vector::from_vec(vec![0.1, 0.2, 0.3]);
+        classifier.w2 = Matrix::from_shape_vec((1, 3), vec![0.7, 0.8, 0.9]).unwrap();
+        classifier.b2 = Vector::from_vec(vec![0.1, 0.2, 0.3]); // Note: only first value used
+
+        // Single sample input
+        let input = Matrix::from_shape_vec((2, 1), vec![1.0, 2.0]).unwrap();
+
+        let result = classifier.forward(&input);
+        assert!(result.is_ok());
+
+        // Manual calculation:
+        // z1 = w1 * x + b1 = [0.1*1.0 + 0.2*2.0 + 0.1, 0.3*1.0 + 0.4*2.0 + 0.2, 0.5*1.0 + 0.6*2.0 + 0.3] = [0.6, 1.3, 2.0]
+        // a1 = ReLU(z1) = [0.6, 1.3, 2.0] (all positive)
+        // z2 = w2 * a1 + b2[0] = 0.7*0.6 + 0.8*1.3 + 0.9*2.0 + 0.1 = 0.42 + 1.04 + 1.8 + 0.1 = 3.36
+        // a2 = sigmoid(z2) = 1/(1+exp(-3.36)) ≈ 0.966
+
+        let expected = Vector::from_vec(vec![0.966]);
+        let output = result.unwrap();
+
+        assert_relative_eq!(output[[0, 0]], expected[0], epsilon = 1e-3);
+    }
+
+    #[test]
+    fn test_forward_with_different_activations() {
+        // Test with tanh in hidden layer and linear in output layer
+        let mut classifier =
+            SingleLayerClassifier::new(2, 3, 0.5, ActivationFn::ReLU, ActivationFn::Sigmoid)
+                .unwrap();
+
+        // Set predetermined weights and biases
+        classifier.w1 = Matrix::from_shape_vec((3, 2), vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6]).unwrap();
+        classifier.b1 = Vector::from_vec(vec![0.1, 0.2, 0.3]);
+        classifier.w2 = Matrix::from_shape_vec((1, 3), vec![0.7, 0.8, 0.9]).unwrap();
+        classifier.b2 = Vector::from_vec(vec![0.1, 0.2, 0.3]);
+
+        let input = Matrix::from_shape_vec((2, 1), vec![1.0, 2.0]).unwrap();
+
+        let result = classifier.forward(&input);
+        assert!(result.is_ok());
+
+        // With different activation functions, the result will differ
+        // This test checks that the forward function runs successfully with various activation functions
+    }
+
+    #[test]
+    fn test_forward_batch_processing() {
+        let classifier =
+            SingleLayerClassifier::new(2, 4, 0.5, ActivationFn::Sigmoid, ActivationFn::ReLU)
+                .unwrap();
+
+        // Batch of 3 samples with 2 features each
+        let input = Matrix::from_shape_vec((2, 3), vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6]).unwrap();
+
+        let result = classifier.forward(&input);
+        assert!(result.is_ok());
+
+        let output = result.unwrap();
+        // Should have one prediction per sample
+        assert_eq!(output.len(), 3);
+
+        // All outputs should be between 0 and 1 (sigmoid activation)
+        for val in output.iter() {
+            assert!(0.0 <= *val && *val <= 1.0);
+        }
+    }
+}
diff --git a/src/prelude.rs b/src/prelude.rs
new file mode 100644
index 0000000..70ab55d
--- /dev/null
+++ b/src/prelude.rs
@@ -0,0 +1,28 @@
+/// The prelude module provides a convenient way to import common components of the library.
+///
+/// # Example
+/// ```rust
+/// use rust_ml::prelude::*;
+///
+/// // Now you can use imported types without full path qualification
+/// // let model = LinearRegression::new(2);
+/// // let optimizer = GradientDescent::new(0.01, 1000);
+/// ```
+// Re-export common types, traits, and functions for convenient imports
+// Users should be able to import everything they need with just `use rust_ml::prelude::*;`
+// Re-export core types
+pub use crate::core::types::{Matrix, Vector};
+
+// Re-export activation functions
+pub use crate::core::activations::activation_functions::ActivationFn;
+
+// Re-export error types
+pub use crate::core::error::ModelError;
+
+// Re-export models
+pub use crate::model::single_layer_classifier::SingleLayerClassifier;
+
+// Re-export builders
+pub mod single_layer_classifier {
+    pub use crate::builders::single_layer_classifier::SingleLayerClassifierBuilder;
+}
